{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawler \u00b6 Web crawler adalah program yang berjalan menggunakan metode tertentu dan secara otomatis mengambil data atau mengumpulkan beberapa data informasi yang ada di dalam suatu website. Web crawler akan mengunjungi alamat website yang dicantumkan, kemudian mengambil dan menyimpan data yang terkandung dalam website tersebut. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling. Install library: \u00b6 install requests : pip install requests Requets adalah modul pada pythin yang dapat digunakan untuk mengirim request HTTP. install beautiful soup 4 : pip install bs4 Beautiful Soup 4 adalah library pada python yang digunkan untuk mengambil data dari file HTML. Crawling: \u00b6 Web yang akan di crawl : http://www.bukabuku.com/ Merubah HTML menjadi object beautiful soup python soup = BeautifulSoup(page.content, 'html.parser') Mendapatkan semua link list title links = soup . findAll ( class_ = 'product_list_title' ) Mengakses setiap halaman for i in links Mendapatkan tag ('a')['href'] pada website yang akan di crawl url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] Mencari tag html untuk content konten = soup . find ( class_ = 'page_content' ) Mencari tag html yang berisi judul buku, kemudian mengambil judul buku judul = konten . find ( class_ = 'product_title' ) . getText () Mencari tag html yang berisi pengarang buku, kemudian mengambil data tersebut pengarang = konten . find ( class_ = 'product_author' ) . getText () Mencari tag html yang berisi link untuk menuju halaman deskripsi detil = konten . find ( class_ = 'product_detail' ) kemudian mengambil data deskripsi dan ditambahkan ke dalam variabel temp deskripsi = des . findAll ( 'p' ) tmp = '' for d in deskripsi : tmp = tmp + d . getText () memasukkan data yang telah diambil ke dalam database conn . execute ( \"INSERT INTO BOOK \\ VALUES (?, ?, ?)\" , ( judul , pengarang , tmp )); code: def crawl ( src ): global c try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , 'html.parser' ) links = soup . findAll ( class_ = 'product_list_title' ) numPages = list () for i in links : try : print ( 'Proses : %.2f ' % (( c / 150 ) * 100 ) + '%' ); c += 0.2 url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] print ( url ) page = requests . get ( url ) soup = BeautifulSoup ( page . content , 'html.parser' ) konten = soup . find ( class_ = 'page_content' ) judul = konten . find ( class_ = 'product_title' ) . getText () print ( judul ) pengarang = konten . find ( class_ = 'product_author' ) . getText () detil = konten . find ( class_ = 'product_detail' ) des = konten . find ( class_ = 'product_description' ) deskripsi = des . findAll ( 'p' ) tmp = '' for d in deskripsi : tmp = tmp + d . getText () conn . execute ( \"INSERT INTO BOOK \\ VALUES (?, ?, ?)\" , ( judul , pengarang , tmp )); except AttributeError : print ( \"\" ) conn . commit () Hasil pada database: \u00b6 Refrensi: https://code.tutsplus.com/id/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211 https://code.tutsplus.com/id/tutorials/using-the-requests-module-in-python--cms-28204 http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6","title":"Home"},{"location":"#web-crawler","text":"Web crawler adalah program yang berjalan menggunakan metode tertentu dan secara otomatis mengambil data atau mengumpulkan beberapa data informasi yang ada di dalam suatu website. Web crawler akan mengunjungi alamat website yang dicantumkan, kemudian mengambil dan menyimpan data yang terkandung dalam website tersebut. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling.","title":"Web Crawler"},{"location":"#install-library","text":"install requests : pip install requests Requets adalah modul pada pythin yang dapat digunakan untuk mengirim request HTTP. install beautiful soup 4 : pip install bs4 Beautiful Soup 4 adalah library pada python yang digunkan untuk mengambil data dari file HTML.","title":"Install library:"},{"location":"#crawling","text":"Web yang akan di crawl : http://www.bukabuku.com/ Merubah HTML menjadi object beautiful soup python soup = BeautifulSoup(page.content, 'html.parser') Mendapatkan semua link list title links = soup . findAll ( class_ = 'product_list_title' ) Mengakses setiap halaman for i in links Mendapatkan tag ('a')['href'] pada website yang akan di crawl url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] Mencari tag html untuk content konten = soup . find ( class_ = 'page_content' ) Mencari tag html yang berisi judul buku, kemudian mengambil judul buku judul = konten . find ( class_ = 'product_title' ) . getText () Mencari tag html yang berisi pengarang buku, kemudian mengambil data tersebut pengarang = konten . find ( class_ = 'product_author' ) . getText () Mencari tag html yang berisi link untuk menuju halaman deskripsi detil = konten . find ( class_ = 'product_detail' ) kemudian mengambil data deskripsi dan ditambahkan ke dalam variabel temp deskripsi = des . findAll ( 'p' ) tmp = '' for d in deskripsi : tmp = tmp + d . getText () memasukkan data yang telah diambil ke dalam database conn . execute ( \"INSERT INTO BOOK \\ VALUES (?, ?, ?)\" , ( judul , pengarang , tmp )); code: def crawl ( src ): global c try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , 'html.parser' ) links = soup . findAll ( class_ = 'product_list_title' ) numPages = list () for i in links : try : print ( 'Proses : %.2f ' % (( c / 150 ) * 100 ) + '%' ); c += 0.2 url = 'http://www.bukabuku.com/' + i . find ( 'a' )[ 'href' ] print ( url ) page = requests . get ( url ) soup = BeautifulSoup ( page . content , 'html.parser' ) konten = soup . find ( class_ = 'page_content' ) judul = konten . find ( class_ = 'product_title' ) . getText () print ( judul ) pengarang = konten . find ( class_ = 'product_author' ) . getText () detil = konten . find ( class_ = 'product_detail' ) des = konten . find ( class_ = 'product_description' ) deskripsi = des . findAll ( 'p' ) tmp = '' for d in deskripsi : tmp = tmp + d . getText () conn . execute ( \"INSERT INTO BOOK \\ VALUES (?, ?, ?)\" , ( judul , pengarang , tmp )); except AttributeError : print ( \"\" ) conn . commit ()","title":"Crawling:"},{"location":"#hasil-pada-database","text":"Refrensi: https://code.tutsplus.com/id/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211 https://code.tutsplus.com/id/tutorials/using-the-requests-module-in-python--cms-28204 http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6","title":"Hasil pada database:"},{"location":"Clustering/","text":"Clustering \u00b6 Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering. Dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. pada code kali ini menggunakan K-Means k-Means \u00b6 K-Means adalah Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi. Menentukan jumlah cluster Mengalokasikan data secara random ke cluster yang ada Menghitung rata-rata setiap cluster dari data yang tergabung di dalamnya Mengalokasikan kembali semua data ke cluster terdekat Mengulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Silhoutte \u00b6 Pengujian model dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. Code clustering: \u00b6 kmeans = KMeans ( n_clusters = 5 , random_state = 0 ) . fit ( fiturBaru ) write_csv ( \"Kluster_label.csv\" , [ kmeans . labels_ ]) s_avg = silhouette_score ( fiturBaru , kmeans . labels_ , random_state = 10 ) print ( s_avg ) for i in range ( len ( kmeans . labels_ )): print ( \"Doc %d =>> cluster %d \" % ( i + 1 , kmeans . labels_ [ i ])) contoh menggunakan k-Means dan shilhotte. pada code diatas membagi cluster sebanyak 5 Refrensi: https://yudiagusta.wordpress.com/clustering/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Clustering"},{"location":"Clustering/#clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering. Dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. pada code kali ini menggunakan K-Means","title":"Clustering"},{"location":"Clustering/#k-means","text":"K-Means adalah Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi. Menentukan jumlah cluster Mengalokasikan data secara random ke cluster yang ada Menghitung rata-rata setiap cluster dari data yang tergabung di dalamnya Mengalokasikan kembali semua data ke cluster terdekat Mengulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy .","title":"k-Means"},{"location":"Clustering/#silhoutte","text":"Pengujian model dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain.","title":"Silhoutte"},{"location":"Clustering/#code-clustering","text":"kmeans = KMeans ( n_clusters = 5 , random_state = 0 ) . fit ( fiturBaru ) write_csv ( \"Kluster_label.csv\" , [ kmeans . labels_ ]) s_avg = silhouette_score ( fiturBaru , kmeans . labels_ , random_state = 10 ) print ( s_avg ) for i in range ( len ( kmeans . labels_ )): print ( \"Doc %d =>> cluster %d \" % ( i + 1 , kmeans . labels_ [ i ])) contoh menggunakan k-Means dan shilhotte. pada code diatas membagi cluster sebanyak 5 Refrensi: https://yudiagusta.wordpress.com/clustering/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Code clustering:"},{"location":"PageRank/","text":"Page Rank \u00b6 Pengertian Page Rank: \u00b6 Page rank adalah nama yang digunakan oleh google link popularitas website. Jumlah PageRank untuk sebuah website ditentukan berdasarkan kuantitas dan kualitas link ke website tersebut dan merupakan nilai probabilitas dalam skala algoritma peringkat Google. Adapun rumus yang diciptakan oleh Google untuk menentukan PageRank merupakan rahasia publik. Dalam Google PageRank, skala yang diukur adalah satu sampai sepuluh (1-10). PageRank inilah yang menentukan peringkat sebuah halaman web dalam mesin pencari (Search Engine) Google. Mesin pencari biasanya menampilkan halaman web yang memiliki kata kunci / keyword yang sesuai dengan pencarian oleh penggunanya. Konsep Page Rank: \u00b6 Algoritme page rank akan memperhitungkan link yang inbound (masuk), dan link yang outbound (keluar) pada setiap halaman. Page rank, memiliki konsep dasar yang sama dengan link popularity , tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound dan outbound link . Pendekatan yang digunakan adalah sebuah halaman akan diangap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking (pagerank) tinggi mengacu ke halaman tersebut. Menggunakan pendekatan page rank, proses terjadi secara rekursif dimana sebuah rangking akan ditentukan oleh rangking dari halaman web yang rangkingnya ditentukan oleh rangking halaman web lain yang memiliki link ke halaman tersebut. Proses ini berarti suatu proses yang berulang (rekursif). Di dunia maya, ada jutaan bahkan milyaran halaman web. Oleh karena itu sebuah rangking halaman web ditentukan dari struktur link dari keseluruhan halaman web yang ada di dunia maya. Perhitungan Page Rank: \u00b6 PR = 0.15 + 0.85 (PR(T1)/C(T1) + PR(T2)/C(T2) + ... + PR(Tn)/C(Tn)) Keterangan: PR(T1) itu nilai pagerank halaman T1 C(T1) itu jumlah link yang keluar dari halaman T1 berlaku seterusnya dari T2 sampai Tn contoh: Dalam kasus di atas keduanya dimulai dengan PR2 (sebenarnya kita tidak tahu nilai PRnya berapa, tapi kita asumsikan saja nilainya sama-sama PR2.... Anda mau ganti nilai keduanya menjadi berapa saja boleh...hasilnya akan sama saja nanti, tidak percaya? Perhitungan pertama.... PR Halaman 1 = 0.15 + 0.85(2/1) = 1.85 PR Halaman 2 = 0.15 + 0.85(1.85/1) = 1.72 Kita lakukan iterasi kedua.... PR Halaman 1 = 0.15 + 0.85(1.72/1) = 1.61 PR Halaman 2 = 0.15 + 0.85(1.61/1) = 1.51 Kita lakukan iterasi ketiga PR Halaman 1 = 0.15 + 0.85(1.51/1) = 1.43 PR Halaman 2 = 0.15 + 0.85(1.43/1) = 1.21 Iterasi terus berlanjut PR Halaman 1 = 0.15 + 0.85(1.21/1) = 1.17 PR Halaman 2 = 0.15 + 0.85(1.17/1) = 1.14 ........ PR Halaman 1 = 0.15 + 0.85(1.14/1) = 1.11 PR Halaman 2 = 0.15 + 0.85(1.11/1) = 1.09 Anda bisa melihat bahwa angkanya semakin mendekati PR1, oleh karena itulah banyak yang beranggapan bahwa nilai default PR suatu halaman adalah PR1. Yang akan merubah itu nantinya adalah berapa banyak link masuk dan link keluar dari halaman tersebut. Jadi mari kita buat perhitungannya sedikit lebih kompleks lagi... Bagaimana jika melibatkan 4 halaman, yaitu 1 homepage, 1 halaman artikel, dan 2 halaman dari website lain.... Lihat gambar di bawah ini. Kita berikan lagi asumsi PR awal untuk keempat halaman ini...terserah berapa saja. Kali ini kita asumsikan semuanya bernilai PR1. Apa yang akan terjadi jika keempat halaman ini dirangkai seperti di atas? Mari kita lihat.... Iterasi pertama.... PR Halaman Beranda = 0.15 + 0.85(\u2153 + 1/1) = 1.28 PR Halaman Artikel = 0.15 + 0.85(1.28/1) = 1.24 PR Halaman Eksternal 1 = 0.15 + 0.85(1.24/3) = 0.50 PR Halaman Eksternal 2 = 0.15 + 0.85(1.24/3) = 0.50 Iterasi kedua dengan nilai di atas PR Halaman Beranda = 0.15 + 0.85(0.50/1 + 1.24/3) = 0.93 PR Halaman Artikel = 0.15 + 0.85(0.93/1) = 0.94 PR Halaman Eksternal 1 = 0.15 + 0.85(0.94/3) = 0.42 PR Halaman Eksternal 2 = 0.15 + 0.85(0.94/3) = 0.42 Setelah 40 kali iterasi maka nilainya menjadi seperti di bawah ini: PR Halaman Beranda = 0.64 PR Halaman Artikel = 0.69 PR Halaman Eksternal 1 = 0.34 PR Halaman Eksternal 2 = 0.34 Apa yang anda lihat? Apakah anda merasa ada yang aneh? Mungkin akan ada yang berkata: \"Total awal PR itu adalah 4x1=4, kenapa setelah diiterasi nilai totalnya kurang dari 4?\" Perlu anda catat bahwa nilai 1 untuk setiap halaman tadi hanyalah sebuah asumsi saja. Anda mau kasih masuk nilai asumsi berapa saja, tetap akan bergerak ke nilai akhir yang seperti ini.... Mungkin ada juga yang bertanya: \"Kenapa halaman beranda yang menerima 2 link masuk dan cuma memberi 1 link keluar nilainya lebih kecil dari halaman artikel yang menerima cuma 1 link masuk tapi memberi 3 link keluar? Bukankah link yang keluar dari halaman artikel lebih banyak dan link masuknya lebih sedikit? Kenapa PR-nya lebih besar?\" Jawabnya sederhana.... Untuk setiap putaran perhitungan, Google Pagerank itu dinilai hanya dari link yang masuk dan bukan dari link yang keluar. Tapi berhubung ini adalah iterasi yang melingkar akhirnya jumlah link yang keluar tadi akan kembali mempengaruhi nilai masukannya. Untuk membuat anda paham, maka anda harus melihat 1 siklus iterasi saja. Beranda nilainya mengecil karena walaupun link masuknya ada dua tapi poin keduanya kecil-kecil. Link yang masuk ke beranda berasal dari 1 halaman artikel yang poinnya terbagi 3 dan 1 halaman eksternal yang memang kecil poinnya. Berbeda dengan halaman artikel, walaupun link masuknya cuma 1, yaitu dari beranda.... tapi hanya ada 1 link keluar dari beranda, jadi poinnya tidak terbagi.... Jadi kalau anda mau meningkatkan Google pagerank suatu halaman, ada tiga hal yang harus anda perhatikan, yaitu: Jumlah backlink, PR masing-masing backlink, dan jumlah link keluar dari masing-masing backlink. Jadi hanya karena link masuk ke beranda lebih banyak daripada link masuk ke artikel belum tentu nilai beranda lebih besar. Lihat dulu nilai poin yang diteruskan oleh masing-masing backlink tadi. Refrensi: http://trikmudahseo.blogspot.com/2012/11/apa-itu-pagerank-dan-cara-menghitung.html https://www.webarq.com/ https://id.wikipedia.org/wiki/PageRank","title":"-Page Rank"},{"location":"PageRank/#page-rank","text":"","title":"Page Rank"},{"location":"PageRank/#pengertian-page-rank","text":"Page rank adalah nama yang digunakan oleh google link popularitas website. Jumlah PageRank untuk sebuah website ditentukan berdasarkan kuantitas dan kualitas link ke website tersebut dan merupakan nilai probabilitas dalam skala algoritma peringkat Google. Adapun rumus yang diciptakan oleh Google untuk menentukan PageRank merupakan rahasia publik. Dalam Google PageRank, skala yang diukur adalah satu sampai sepuluh (1-10). PageRank inilah yang menentukan peringkat sebuah halaman web dalam mesin pencari (Search Engine) Google. Mesin pencari biasanya menampilkan halaman web yang memiliki kata kunci / keyword yang sesuai dengan pencarian oleh penggunanya.","title":"Pengertian Page Rank:"},{"location":"PageRank/#konsep-page-rank","text":"Algoritme page rank akan memperhitungkan link yang inbound (masuk), dan link yang outbound (keluar) pada setiap halaman. Page rank, memiliki konsep dasar yang sama dengan link popularity , tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound dan outbound link . Pendekatan yang digunakan adalah sebuah halaman akan diangap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking (pagerank) tinggi mengacu ke halaman tersebut. Menggunakan pendekatan page rank, proses terjadi secara rekursif dimana sebuah rangking akan ditentukan oleh rangking dari halaman web yang rangkingnya ditentukan oleh rangking halaman web lain yang memiliki link ke halaman tersebut. Proses ini berarti suatu proses yang berulang (rekursif). Di dunia maya, ada jutaan bahkan milyaran halaman web. Oleh karena itu sebuah rangking halaman web ditentukan dari struktur link dari keseluruhan halaman web yang ada di dunia maya.","title":"Konsep Page Rank:"},{"location":"PageRank/#perhitungan-page-rank","text":"PR = 0.15 + 0.85 (PR(T1)/C(T1) + PR(T2)/C(T2) + ... + PR(Tn)/C(Tn)) Keterangan: PR(T1) itu nilai pagerank halaman T1 C(T1) itu jumlah link yang keluar dari halaman T1 berlaku seterusnya dari T2 sampai Tn contoh: Dalam kasus di atas keduanya dimulai dengan PR2 (sebenarnya kita tidak tahu nilai PRnya berapa, tapi kita asumsikan saja nilainya sama-sama PR2.... Anda mau ganti nilai keduanya menjadi berapa saja boleh...hasilnya akan sama saja nanti, tidak percaya? Perhitungan pertama.... PR Halaman 1 = 0.15 + 0.85(2/1) = 1.85 PR Halaman 2 = 0.15 + 0.85(1.85/1) = 1.72 Kita lakukan iterasi kedua.... PR Halaman 1 = 0.15 + 0.85(1.72/1) = 1.61 PR Halaman 2 = 0.15 + 0.85(1.61/1) = 1.51 Kita lakukan iterasi ketiga PR Halaman 1 = 0.15 + 0.85(1.51/1) = 1.43 PR Halaman 2 = 0.15 + 0.85(1.43/1) = 1.21 Iterasi terus berlanjut PR Halaman 1 = 0.15 + 0.85(1.21/1) = 1.17 PR Halaman 2 = 0.15 + 0.85(1.17/1) = 1.14 ........ PR Halaman 1 = 0.15 + 0.85(1.14/1) = 1.11 PR Halaman 2 = 0.15 + 0.85(1.11/1) = 1.09 Anda bisa melihat bahwa angkanya semakin mendekati PR1, oleh karena itulah banyak yang beranggapan bahwa nilai default PR suatu halaman adalah PR1. Yang akan merubah itu nantinya adalah berapa banyak link masuk dan link keluar dari halaman tersebut. Jadi mari kita buat perhitungannya sedikit lebih kompleks lagi... Bagaimana jika melibatkan 4 halaman, yaitu 1 homepage, 1 halaman artikel, dan 2 halaman dari website lain.... Lihat gambar di bawah ini. Kita berikan lagi asumsi PR awal untuk keempat halaman ini...terserah berapa saja. Kali ini kita asumsikan semuanya bernilai PR1. Apa yang akan terjadi jika keempat halaman ini dirangkai seperti di atas? Mari kita lihat.... Iterasi pertama.... PR Halaman Beranda = 0.15 + 0.85(\u2153 + 1/1) = 1.28 PR Halaman Artikel = 0.15 + 0.85(1.28/1) = 1.24 PR Halaman Eksternal 1 = 0.15 + 0.85(1.24/3) = 0.50 PR Halaman Eksternal 2 = 0.15 + 0.85(1.24/3) = 0.50 Iterasi kedua dengan nilai di atas PR Halaman Beranda = 0.15 + 0.85(0.50/1 + 1.24/3) = 0.93 PR Halaman Artikel = 0.15 + 0.85(0.93/1) = 0.94 PR Halaman Eksternal 1 = 0.15 + 0.85(0.94/3) = 0.42 PR Halaman Eksternal 2 = 0.15 + 0.85(0.94/3) = 0.42 Setelah 40 kali iterasi maka nilainya menjadi seperti di bawah ini: PR Halaman Beranda = 0.64 PR Halaman Artikel = 0.69 PR Halaman Eksternal 1 = 0.34 PR Halaman Eksternal 2 = 0.34 Apa yang anda lihat? Apakah anda merasa ada yang aneh? Mungkin akan ada yang berkata: \"Total awal PR itu adalah 4x1=4, kenapa setelah diiterasi nilai totalnya kurang dari 4?\" Perlu anda catat bahwa nilai 1 untuk setiap halaman tadi hanyalah sebuah asumsi saja. Anda mau kasih masuk nilai asumsi berapa saja, tetap akan bergerak ke nilai akhir yang seperti ini.... Mungkin ada juga yang bertanya: \"Kenapa halaman beranda yang menerima 2 link masuk dan cuma memberi 1 link keluar nilainya lebih kecil dari halaman artikel yang menerima cuma 1 link masuk tapi memberi 3 link keluar? Bukankah link yang keluar dari halaman artikel lebih banyak dan link masuknya lebih sedikit? Kenapa PR-nya lebih besar?\" Jawabnya sederhana.... Untuk setiap putaran perhitungan, Google Pagerank itu dinilai hanya dari link yang masuk dan bukan dari link yang keluar. Tapi berhubung ini adalah iterasi yang melingkar akhirnya jumlah link yang keluar tadi akan kembali mempengaruhi nilai masukannya. Untuk membuat anda paham, maka anda harus melihat 1 siklus iterasi saja. Beranda nilainya mengecil karena walaupun link masuknya ada dua tapi poin keduanya kecil-kecil. Link yang masuk ke beranda berasal dari 1 halaman artikel yang poinnya terbagi 3 dan 1 halaman eksternal yang memang kecil poinnya. Berbeda dengan halaman artikel, walaupun link masuknya cuma 1, yaitu dari beranda.... tapi hanya ada 1 link keluar dari beranda, jadi poinnya tidak terbagi.... Jadi kalau anda mau meningkatkan Google pagerank suatu halaman, ada tiga hal yang harus anda perhatikan, yaitu: Jumlah backlink, PR masing-masing backlink, dan jumlah link keluar dari masing-masing backlink. Jadi hanya karena link masuk ke beranda lebih banyak daripada link masuk ke artikel belum tentu nilai beranda lebih besar. Lihat dulu nilai poin yang diteruskan oleh masing-masing backlink tadi. Refrensi: http://trikmudahseo.blogspot.com/2012/11/apa-itu-pagerank-dan-cara-menghitung.html https://www.webarq.com/ https://id.wikipedia.org/wiki/PageRank","title":"Perhitungan Page Rank:"},{"location":"Preprocessing/","text":"Preprocessing \u00b6 Preprocessing adalah proses yang dilaukan untuk membuat data mentah menjadi data yang berkualitas. Preprocessing bertujuan untuk membersihkan data, integrasi data, transformasi data, pengurangan data, dan diekstrasi data. Seleksi Fitur \u00b6 Seleksi fitur adalah suatu tahapan preposses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan untuk mempengaruhi hasil klasifikasi. pada seleksi fitur kali ini saya menggunakan Pearson Correlation. Pearson Correlation \u00b6 Pearson correlation Merupakan salah satu korelasi yang digunkan untuk menghitung kekuatan dan arah hubungan linier dari dua variabel. Dua Variabel berkolerasi jika perubahan satu variabel di ikuti dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arahyang sebaliknya. import Numpy import numpy as np def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u < len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] kataBaru = katadasar [: u + 1 ] v = u while v < len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value < threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) kataBaru = np . hstack (( kataBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = kataBaru if u % 50 == 0 : print ( \"proses : \" , u , data . shape ) u += 1 return data , kataBaru Refrensi: https://repository.ipb.ac.id/handle/123456789/14020 https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Preprocessing"},{"location":"Preprocessing/#preprocessing","text":"Preprocessing adalah proses yang dilaukan untuk membuat data mentah menjadi data yang berkualitas. Preprocessing bertujuan untuk membersihkan data, integrasi data, transformasi data, pengurangan data, dan diekstrasi data.","title":"Preprocessing"},{"location":"Preprocessing/#seleksi-fitur","text":"Seleksi fitur adalah suatu tahapan preposses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan untuk mempengaruhi hasil klasifikasi. pada seleksi fitur kali ini saya menggunakan Pearson Correlation.","title":"Seleksi Fitur"},{"location":"Preprocessing/#pearson-correlation","text":"Pearson correlation Merupakan salah satu korelasi yang digunkan untuk menghitung kekuatan dan arah hubungan linier dari dua variabel. Dua Variabel berkolerasi jika perubahan satu variabel di ikuti dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arahyang sebaliknya. import Numpy import numpy as np def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u < len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] kataBaru = katadasar [: u + 1 ] v = u while v < len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value < threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) kataBaru = np . hstack (( kataBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = kataBaru if u % 50 == 0 : print ( \"proses : \" , u , data . shape ) u += 1 return data , kataBaru Refrensi: https://repository.ipb.ac.id/handle/123456789/14020 https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Pearson Correlation"},{"location":"Text Extraction/","text":"Text Extraction \u00b6 Text Extraction adalah melakukan ekstrasi pada text. Melakukan proses (1) Stopword Remove, (2) Stemming, (3) Tokenisasi. Import library \u00b6 import stemmer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory Memanggil library stemmer, dan stopwordremover Proses pada text extraction \u00b6 Stopword Remove Stopword adalah pengabaian kata dalam pemrosesan, kata yang diabaikan akan disimpan ke dalam stop list. SWfactory = StopWordRemoverFactory () stopword = SWfactory . create_stop_word_remover () Stemming Stemming adalah proses untuk menentukan kata dasar dengan menggunakan cara menghilangkan huruf imbuhan pada kata, baik imbuhan pada awal, sisipan, dan akhiran akan dihapuskan. Sfactory = StemmerFactory () stemmer = Sfactory . create_stemmer () Tokenisasi Tokenisasi adalah suatu proses pemisahan kata, simbol, frase, dan entitas lainnya pada sebuah teks yang kemudian akan dianalisa. def tokenisasi ( txt , ngram = 1 ): token = [] start = 0 end = ngram txtSplit = txt . split () while end <= len ( txtSplit ): tmp = txtSplit [ start : end ] frase = '' for i in tmp : frase += i + ' ' token . append ( frase ) end += 1 ; start += 1 ; return token Hasil Stopword: \u00b6 Refrensi: https://ismaildoanxz.wordpress.com/2009/12/04/pengertian-stemming/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Text Extraction"},{"location":"Text Extraction/#text-extraction","text":"Text Extraction adalah melakukan ekstrasi pada text. Melakukan proses (1) Stopword Remove, (2) Stemming, (3) Tokenisasi.","title":"Text Extraction"},{"location":"Text Extraction/#import-library","text":"import stemmer from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory Memanggil library stemmer, dan stopwordremover","title":"Import library"},{"location":"Text Extraction/#proses-pada-text-extraction","text":"Stopword Remove Stopword adalah pengabaian kata dalam pemrosesan, kata yang diabaikan akan disimpan ke dalam stop list. SWfactory = StopWordRemoverFactory () stopword = SWfactory . create_stop_word_remover () Stemming Stemming adalah proses untuk menentukan kata dasar dengan menggunakan cara menghilangkan huruf imbuhan pada kata, baik imbuhan pada awal, sisipan, dan akhiran akan dihapuskan. Sfactory = StemmerFactory () stemmer = Sfactory . create_stemmer () Tokenisasi Tokenisasi adalah suatu proses pemisahan kata, simbol, frase, dan entitas lainnya pada sebuah teks yang kemudian akan dianalisa. def tokenisasi ( txt , ngram = 1 ): token = [] start = 0 end = ngram txtSplit = txt . split () while end <= len ( txtSplit ): tmp = txtSplit [ start : end ] frase = '' for i in tmp : frase += i + ' ' token . append ( frase ) end += 1 ; start += 1 ; return token","title":"Proses pada text extraction"},{"location":"Text Extraction/#hasil-stopword","text":"Refrensi: https://ismaildoanxz.wordpress.com/2009/12/04/pengertian-stemming/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Hasil Stopword:"},{"location":"Tf-idf/","text":"Tf-idf \u00b6 tf-idf adalah algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada suatu dokumen. Rumus nilai TF = jumlah frekuensi kata terpilih / jumlah kata, kemudian IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih). Contoh IDF: \u00b6 Koleksi dokumen: Yang domisili di Cibinong & sekitarnya siang ini kita olahraga bareng Tau belum bogor jadi kandidat the most loveable city in the world? Salam olahraga Minggu semangat..!!! @ Taman Bogor term: olahraga, taman DF: Diketahui: N = 3 DF (olahraga, d) = 3 (dokumen 1, 2, dan 3) DF (taman, d) = 1 (dokumen 1) Maka IDF dari term \"olahraga\" dan \"taman\" Term DF IDF olahraga 3 0 taman 1 0.477121 Code pada web crawl: \u00b6 vectorizer = TfidfVectorizer () tfidf_matrix = vectorizer . fit_transform ( corpus ) feature_name = vectorizer . get_feature_names () write_csv ( \"tfidf.csv\" , [ feature_name ]) write_csv ( \"tfidf.csv\" , tfidf_matrix . toarray (), 'a' ) Refrensi: https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/","title":"Tf-idf"},{"location":"Tf-idf/#tf-idf","text":"tf-idf adalah algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada suatu dokumen. Rumus nilai TF = jumlah frekuensi kata terpilih / jumlah kata, kemudian IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih).","title":"Tf-idf"},{"location":"Tf-idf/#contoh-idf","text":"Koleksi dokumen: Yang domisili di Cibinong & sekitarnya siang ini kita olahraga bareng Tau belum bogor jadi kandidat the most loveable city in the world? Salam olahraga Minggu semangat..!!! @ Taman Bogor term: olahraga, taman DF: Diketahui: N = 3 DF (olahraga, d) = 3 (dokumen 1, 2, dan 3) DF (taman, d) = 1 (dokumen 1) Maka IDF dari term \"olahraga\" dan \"taman\" Term DF IDF olahraga 3 0 taman 1 0.477121","title":"Contoh IDF:"},{"location":"Tf-idf/#code-pada-web-crawl","text":"vectorizer = TfidfVectorizer () tfidf_matrix = vectorizer . fit_transform ( corpus ) feature_name = vectorizer . get_feature_names () write_csv ( \"tfidf.csv\" , [ feature_name ]) write_csv ( \"tfidf.csv\" , tfidf_matrix . toarray (), 'a' ) Refrensi: https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/","title":"Code pada web crawl:"},{"location":"VSM/","text":"VSM(Vector Space Model) \u00b6 VSM adalah suatu metode yang digunakan untuk meghitung kemiripan antar kata pada suatu dokumen. Menghitung kata pada satu string \u00b6 def countWord ( txt ): d = dict () for i in txt . split (): if d . get ( i ) == None : d [ i ] = txt . count ( i ) return d fungsi ini melakukan perhitungan kata pada string yang ada pada txt Membuat vsm: \u00b6 def add_row_VSM ( d ): VSM . append ([]) for i in VSM [ 0 ]: if d . get ( i ) == None : VSM [ - 1 ] . append ( 0 ) else : VSM [ - 1 ] . append ( d . pop ( i )); for i in d : VSM [ 0 ] . append ( i ) for j in range ( 1 , len ( VSM ) - 1 ): VSM [ j ] . insert ( - 2 , 0 ) VSM [ - 1 ] . append ( d . get ( i )) pada fungsi ini digunkan untuk memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya, dan memasukkan kata baru. Refrensi: https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/","title":"VSM"},{"location":"VSM/#vsmvector-space-model","text":"VSM adalah suatu metode yang digunakan untuk meghitung kemiripan antar kata pada suatu dokumen.","title":"VSM(Vector Space Model)"},{"location":"VSM/#menghitung-kata-pada-satu-string","text":"def countWord ( txt ): d = dict () for i in txt . split (): if d . get ( i ) == None : d [ i ] = txt . count ( i ) return d fungsi ini melakukan perhitungan kata pada string yang ada pada txt","title":"Menghitung kata pada satu string"},{"location":"VSM/#membuat-vsm","text":"def add_row_VSM ( d ): VSM . append ([]) for i in VSM [ 0 ]: if d . get ( i ) == None : VSM [ - 1 ] . append ( 0 ) else : VSM [ - 1 ] . append ( d . pop ( i )); for i in d : VSM [ 0 ] . append ( i ) for j in range ( 1 , len ( VSM ) - 1 ): VSM [ j ] . insert ( - 2 , 0 ) VSM [ - 1 ] . append ( d . get ( i )) pada fungsi ini digunkan untuk memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya, dan memasukkan kata baru. Refrensi: https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/","title":"Membuat vsm:"}]}