{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawler Web crawler adalah program yang berjalan menggunakan metode tertentu dan secara otomatis mengambil data atau mengumpulkan beberapa data informasi yang ada di dalam suatu website. Web crawler akan mengunjungi alamat website yang dicantumkan, kemudian mengambil dan menyimpan data yang terkandung dalam website tersebut. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling. Install library: install requests : pip install requests Requets adalah modul pada pythin yang dapat digunakan untuk mengirim request HTTP. install beautiful soup 4 : pip install bs4 Beautiful Soup 4 adalah library pada python yang digunkan untuk mengambil data dari file HTML. Crawling: Web yang akan di crawl : http://www.bukabuku.com/ Merubah HTML menjadi object beautiful soup python soup = BeautifulSoup(page.content, 'html.parser') Mendapatkan semua link list title python links = soup.findAll(class_='product_list_title') Mengakses setiap halaman python for i in links Mendapatkan tag ('a')['href'] pada website yang akan di crawl python url = 'http://www.bukabuku.com/' + i.find ('a')['href']url = 'http://www.bukabuku.com/' + i.find ('a')['href'] Mencari tag html untuk content python konten = soup.find(class_='page_content') Mencari tag html yang berisi judul buku, kemudian mengambil judul buku python judul = konten.find(class_='product_title').getText() Mencari tag html yang berisi pengarang buku, kemudian mengambil data tersebut python pengarang = konten.find(class_='product_author').getText() Mencari tag html yang berisi link untuk menuju halaman deskripsi python detil = konten.find(class_='product_detail') kemudian mengambil data deskripsi dan ditambahkan ke dalam variabel temp ```python deskripsi = des.findAll('p') tmp = '' for d in deskripsi: tmp = tmp + d.getText() ``` memasukkan data yang telah diambil ke dalam database ```python conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, pengarang, tmp)); ``` code: def crawl(src): global c try : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product_list_title') numPages = list() for i in links: try : print ('Proses : %.2f' %((c/150)*100) + '%'); c+=0.2 url = 'http://www.bukabuku.com/' + i.find ('a')['href'] print(url) page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='page_content') judul = konten.find(class_='product_title').getText() print(judul) pengarang = konten.find(class_='product_author').getText() detil = konten.find(class_='product_detail') des = konten.find(class_= 'product_description') deskripsi = des.findAll('p') tmp = '' for d in deskripsi: tmp = tmp + d.getText() conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, pengarang, tmp)); except AttributeError: print(\"\") conn.commit() Hasil pada database: Refrensi: https://code.tutsplus.com/id/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211 https://code.tutsplus.com/id/tutorials/using-the-requests-module-in-python--cms-28204 http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6","title":"Home"},{"location":"#web-crawler","text":"Web crawler adalah program yang berjalan menggunakan metode tertentu dan secara otomatis mengambil data atau mengumpulkan beberapa data informasi yang ada di dalam suatu website. Web crawler akan mengunjungi alamat website yang dicantumkan, kemudian mengambil dan menyimpan data yang terkandung dalam website tersebut. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling.","title":"Web Crawler"},{"location":"#install-library","text":"install requests : pip install requests Requets adalah modul pada pythin yang dapat digunakan untuk mengirim request HTTP. install beautiful soup 4 : pip install bs4 Beautiful Soup 4 adalah library pada python yang digunkan untuk mengambil data dari file HTML.","title":"Install library:"},{"location":"#crawling","text":"Web yang akan di crawl : http://www.bukabuku.com/ Merubah HTML menjadi object beautiful soup python soup = BeautifulSoup(page.content, 'html.parser') Mendapatkan semua link list title python links = soup.findAll(class_='product_list_title') Mengakses setiap halaman python for i in links Mendapatkan tag ('a')['href'] pada website yang akan di crawl python url = 'http://www.bukabuku.com/' + i.find ('a')['href']url = 'http://www.bukabuku.com/' + i.find ('a')['href'] Mencari tag html untuk content python konten = soup.find(class_='page_content') Mencari tag html yang berisi judul buku, kemudian mengambil judul buku python judul = konten.find(class_='product_title').getText() Mencari tag html yang berisi pengarang buku, kemudian mengambil data tersebut python pengarang = konten.find(class_='product_author').getText() Mencari tag html yang berisi link untuk menuju halaman deskripsi python detil = konten.find(class_='product_detail') kemudian mengambil data deskripsi dan ditambahkan ke dalam variabel temp ```python deskripsi = des.findAll('p') tmp = '' for d in deskripsi: tmp = tmp + d.getText() ``` memasukkan data yang telah diambil ke dalam database ```python conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, pengarang, tmp)); ``` code: def crawl(src): global c try : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product_list_title') numPages = list() for i in links: try : print ('Proses : %.2f' %((c/150)*100) + '%'); c+=0.2 url = 'http://www.bukabuku.com/' + i.find ('a')['href'] print(url) page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='page_content') judul = konten.find(class_='product_title').getText() print(judul) pengarang = konten.find(class_='product_author').getText() detil = konten.find(class_='product_detail') des = konten.find(class_= 'product_description') deskripsi = des.findAll('p') tmp = '' for d in deskripsi: tmp = tmp + d.getText() conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, pengarang, tmp)); except AttributeError: print(\"\") conn.commit()","title":"Crawling:"},{"location":"#hasil-pada-database","text":"Refrensi: https://code.tutsplus.com/id/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211 https://code.tutsplus.com/id/tutorials/using-the-requests-module-in-python--cms-28204 http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://datascience.or.id/article/Mengenal-BeautifulSoup-Untuk-WebScraping-5a8fa6e6","title":"Hasil pada database:"},{"location":"Clustering/","text":"Clustering Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering. Dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. pada code kali ini menggunakan K-Means k-Means K-Means adalah Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi. Menentukan jumlah cluster Mengalokasikan data secara random ke cluster yang ada Menghitung rata-rata setiap cluster dari data yang tergabung di dalamnya Mengalokasikan kembali semua data ke cluster terdekat Mengulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Silhoutte Pengujian model dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. Code clustering: kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) contoh menggunakan k-Means dan shilhotte. pada code diatas membagi cluster sebanyak 5 Refrensi: https://yudiagusta.wordpress.com/clustering/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Clustering"},{"location":"Clustering/#clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering. Dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. pada code kali ini menggunakan K-Means","title":"Clustering"},{"location":"Clustering/#k-means","text":"K-Means adalah Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi. Menentukan jumlah cluster Mengalokasikan data secara random ke cluster yang ada Menghitung rata-rata setiap cluster dari data yang tergabung di dalamnya Mengalokasikan kembali semua data ke cluster terdekat Mengulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy .","title":"k-Means"},{"location":"Clustering/#silhoutte","text":"Pengujian model dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain.","title":"Silhoutte"},{"location":"Clustering/#code-clustering","text":"kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) contoh menggunakan k-Means dan shilhotte. pada code diatas membagi cluster sebanyak 5 Refrensi: https://yudiagusta.wordpress.com/clustering/ http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html","title":"Code clustering:"},{"location":"Preprocessing/","text":"Preprocessing Preprocessing adalah proses yang dilaukan untuk membuat data mentah menjadi data yang berkualitas. Preprocessing bertujuan untuk membersihkan data, integrasi data, transformasi data, pengurangan data, dan diekstrasi data. Seleksi Fitur Seleksi fitur adalah suatu tahapan preposses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan untuk mempengaruhi hasil klasifikasi. pada seleksi fitur kali ini saya menggunakan Pearson Correlation. Pearson Correlation Pearson correlation Merupakan salah satu korelasi yang digunkan untuk menghitung kekuatan dan arah hubungan linier dari dua variabel. Dua Variabel berkolerasi jika perubahan satu variabel di ikuti dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arahyang sebaliknya. import Numpy import numpy as np def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, kataBaru Refrensi: https://repository.ipb.ac.id/handle/123456789/14020 https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Preprocessing"},{"location":"Preprocessing/#preprocessing","text":"Preprocessing adalah proses yang dilaukan untuk membuat data mentah menjadi data yang berkualitas. Preprocessing bertujuan untuk membersihkan data, integrasi data, transformasi data, pengurangan data, dan diekstrasi data.","title":"Preprocessing"},{"location":"Preprocessing/#seleksi-fitur","text":"Seleksi fitur adalah suatu tahapan preposses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan untuk mempengaruhi hasil klasifikasi. pada seleksi fitur kali ini saya menggunakan Pearson Correlation.","title":"Seleksi Fitur"},{"location":"Preprocessing/#pearson-correlation","text":"Pearson correlation Merupakan salah satu korelasi yang digunkan untuk menghitung kekuatan dan arah hubungan linier dari dua variabel. Dua Variabel berkolerasi jika perubahan satu variabel di ikuti dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arahyang sebaliknya. import Numpy import numpy as np def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, kataBaru Refrensi: https://repository.ipb.ac.id/handle/123456789/14020 https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Pearson Correlation"},{"location":"Text Extraction/","text":"Text Extraction Text Extraction adalah melakukan ekstrasi pada text. Melakukan proses (1) Stopword Remove, (2) Stemming, (3) Tokenisasi. Import library import stemmer python from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory Memanggil library stemmer, dan stopwordremover Proses pada text extraction Stopword Remove Stopword adalah pengabaian kata dalam pemrosesan, kata yang diabaikan akan disimpan ke dalam stop list. python SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Stemming Stemming adalah proses untuk menentukan kata dasar dengan menggunakan cara menghilangkan huruf imbuhan pada kata, baik imbuhan pada awal, sisipan, dan akhiran akan dihapuskan. python Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() Tokenisasi Tokenisasi adalah suatu proses pemisahan kata, simbol, frase, dan entitas lainnya pada sebuah teks yang kemudian akan dianalisa. python def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token Hasil Stopword: Refrensi: https://ismaildoanxz.wordpress.com/2009/12/04/pengertian-stemming/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Text Extraction"},{"location":"Text Extraction/#text-extraction","text":"Text Extraction adalah melakukan ekstrasi pada text. Melakukan proses (1) Stopword Remove, (2) Stemming, (3) Tokenisasi.","title":"Text Extraction"},{"location":"Text Extraction/#import-library","text":"import stemmer python from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory Memanggil library stemmer, dan stopwordremover","title":"Import library"},{"location":"Text Extraction/#proses-pada-text-extraction","text":"Stopword Remove Stopword adalah pengabaian kata dalam pemrosesan, kata yang diabaikan akan disimpan ke dalam stop list. python SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Stemming Stemming adalah proses untuk menentukan kata dasar dengan menggunakan cara menghilangkan huruf imbuhan pada kata, baik imbuhan pada awal, sisipan, dan akhiran akan dihapuskan. python Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() Tokenisasi Tokenisasi adalah suatu proses pemisahan kata, simbol, frase, dan entitas lainnya pada sebuah teks yang kemudian akan dianalisa. python def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token","title":"Proses pada text extraction"},{"location":"Text Extraction/#hasil-stopword","text":"Refrensi: https://ismaildoanxz.wordpress.com/2009/12/04/pengertian-stemming/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Hasil Stopword:"},{"location":"Tf-idf/","text":"Tf-idf tf-idf adalah algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada suatu dokumen. Rumus nilai TF = jumlah frekuensi kata terpilih / jumlah kata, kemudian IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih). Contoh IDF: Koleksi dokumen: Yang domisili di Cibinong & sekitarnya siang ini kita olahraga bareng Tau belum bogor jadi kandidat the most loveable city in the world? Salam olahraga Minggu semangat..!!! @ Taman Bogor term: olahraga, taman DF: Diketahui: N = 3 DF (olahraga, d) = 3 (dokumen 1, 2, dan 3) DF (taman, d) = 1 (dokumen 1) Maka IDF dari term \"olahraga\" dan \"taman\" Term DF IDF olahraga 3 0 taman 1 0.477121 Code pada web crawl: vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') Refrensi: https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/","title":"Tf-idf"},{"location":"Tf-idf/#tf-idf","text":"tf-idf adalah algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase atau kalimat pada suatu dokumen. Rumus nilai TF = jumlah frekuensi kata terpilih / jumlah kata, kemudian IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih).","title":"Tf-idf"},{"location":"Tf-idf/#contoh-idf","text":"Koleksi dokumen: Yang domisili di Cibinong & sekitarnya siang ini kita olahraga bareng Tau belum bogor jadi kandidat the most loveable city in the world? Salam olahraga Minggu semangat..!!! @ Taman Bogor term: olahraga, taman DF: Diketahui: N = 3 DF (olahraga, d) = 3 (dokumen 1, 2, dan 3) DF (taman, d) = 1 (dokumen 1) Maka IDF dari term \"olahraga\" dan \"taman\" Term DF IDF olahraga 3 0 taman 1 0.477121","title":"Contoh IDF:"},{"location":"Tf-idf/#code-pada-web-crawl","text":"vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') Refrensi: https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/","title":"Code pada web crawl:"},{"location":"VSM/","text":"VSM(Vector Space Model) VSM adalah suatu metode yang digunakan untuk meghitung kemiripan antar kata pada suatu dokumen. Menghitung kata pada satu string def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d fungsi ini melakukan perhitungan kata pada string yang ada pada txt Membuat vsm: def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) pada fungsi ini digunkan untuk memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya, dan memasukkan kata baru. Refrensi: https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/","title":"VSM"},{"location":"VSM/#vsmvector-space-model","text":"VSM adalah suatu metode yang digunakan untuk meghitung kemiripan antar kata pada suatu dokumen.","title":"VSM(Vector Space Model)"},{"location":"VSM/#menghitung-kata-pada-satu-string","text":"def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d fungsi ini melakukan perhitungan kata pada string yang ada pada txt","title":"Menghitung kata pada satu string"},{"location":"VSM/#membuat-vsm","text":"def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) pada fungsi ini digunkan untuk memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya, dan memasukkan kata baru. Refrensi: https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/","title":"Membuat vsm:"}]}